{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68f8027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import os, shutil\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "from itertools import islice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28d57285",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_dir = 'C:/Users/pauli/Documents/M2/ML et DL/projet/Git/DamageDetection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "907b809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_damage_dir = original_dataset_dir + '/train_another/damage'\n",
    "validation_damage_dir = original_dataset_dir + '/validation_another/damage'\n",
    "test_damage_dir = original_dataset_dir + '/test/damage'\n",
    "\n",
    "train_nodamage_dir = original_dataset_dir + '/train_another/no_damage'\n",
    "validation_nodamage_dir = original_dataset_dir + '/validation_another/no_damage'\n",
    "test_nodamage_dir = original_dataset_dir + '/test/no_damage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb3847ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training damage images:  5000\n",
      "total validation damage images:  1000\n",
      "total test damage images:  1000\n",
      "total training no damage images:  5000\n",
      "total validation no damage images:  1000\n",
      "total test no damage images:  1000\n"
     ]
    }
   ],
   "source": [
    "print('total training damage images: ',len(os.listdir(train_damage_dir)))\n",
    "print('total validation damage images: ',len(os.listdir(validation_damage_dir)))\n",
    "print('total test damage images: ',len(os.listdir(test_damage_dir)))\n",
    "\n",
    "print('total training no damage images: ',len(os.listdir(train_nodamage_dir)))\n",
    "print('total validation no damage images: ',len(os.listdir(validation_nodamage_dir)))\n",
    "print('total test no damage images: ',len(os.listdir(test_nodamage_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1eced00",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "    \n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, (3,3))\n",
    "        self.pool1 = nn.MaxPool2d((2,2))\n",
    "        self.conv2 = nn.Conv2d(32, 64, (3,3))\n",
    "        self.pool2 = nn.MaxPool2d((2,2))\n",
    "        self.conv3 = nn.Conv2d(64, 128, (3,3))\n",
    "        self.pool3 = nn.MaxPool2d((2,2))\n",
    "        self.conv4 = nn.Conv2d(128, 128, (3,3))\n",
    "        self.pool4 = nn.MaxPool2d((2,2))\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 128 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ac90d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b637e282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 148, 148]             896\n",
      "         MaxPool2d-2           [-1, 32, 74, 74]               0\n",
      "            Conv2d-3           [-1, 64, 72, 72]          18,496\n",
      "         MaxPool2d-4           [-1, 64, 36, 36]               0\n",
      "            Conv2d-5          [-1, 128, 34, 34]          73,856\n",
      "         MaxPool2d-6          [-1, 128, 17, 17]               0\n",
      "            Conv2d-7          [-1, 128, 15, 15]         147,584\n",
      "         MaxPool2d-8            [-1, 128, 7, 7]               0\n",
      "            Linear-9                  [-1, 512]       3,211,776\n",
      "           Linear-10                    [-1, 1]             513\n",
      "================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.26\n",
      "Forward/backward pass size (MB): 11.53\n",
      "Params size (MB): 13.17\n",
      "Estimated Total Size (MB): 24.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "model = CNN()\n",
    "torchsummary.summary(model, (3,150,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efe61c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.BCELoss() #torch.nn.functional.binary_cross_entropy()\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a419d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    #transforms.Normalize((0,), (1/255,))\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder('C:/Users/pauli/Documents/M2/ML et DL/projet/Git/DamageDetection/train_another', transform=train_datagen)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "validation_datagen = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    #transforms.Normalize((0,), (1/255,))\n",
    "])\n",
    "\n",
    "val_dataset = ImageFolder('C:/Users/pauli/Documents/M2/ML et DL/projet/Git/DamageDetection/validation_another', transform=validation_datagen)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09c09d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "print('Found {} images belonging to {} classes.'.format(len(train_dataset), len(train_dataset.classes)))\n",
    "print('Found {} images belonging to {} classes.'.format(len(val_dataset), len(val_dataset.classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43792956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.3465, Train Accuracy: 0.6880, Validation Loss: 0.5962, Validation Accuracy: 0.7150\n",
      "Epoch [2/50], Train Loss: 0.2742, Train Accuracy: 0.8495, Validation Loss: 0.3180, Validation Accuracy: 0.9050\n",
      "Epoch [3/50], Train Loss: 0.3879, Train Accuracy: 0.8660, Validation Loss: 0.2508, Validation Accuracy: 0.9220\n",
      "Epoch [4/50], Train Loss: 0.1956, Train Accuracy: 0.8835, Validation Loss: 0.2429, Validation Accuracy: 0.9130\n",
      "Epoch [5/50], Train Loss: 0.6793, Train Accuracy: 0.8970, Validation Loss: 0.2817, Validation Accuracy: 0.9100\n",
      "Epoch [6/50], Train Loss: 0.1135, Train Accuracy: 0.9020, Validation Loss: 0.2132, Validation Accuracy: 0.9420\n",
      "Epoch [7/50], Train Loss: 0.1039, Train Accuracy: 0.9090, Validation Loss: 0.2694, Validation Accuracy: 0.9040\n",
      "Epoch [8/50], Train Loss: 0.2670, Train Accuracy: 0.9110, Validation Loss: 0.2113, Validation Accuracy: 0.9420\n",
      "Epoch [9/50], Train Loss: 0.2011, Train Accuracy: 0.9170, Validation Loss: 0.2035, Validation Accuracy: 0.9410\n",
      "Epoch [10/50], Train Loss: 0.2041, Train Accuracy: 0.9150, Validation Loss: 0.2003, Validation Accuracy: 0.9360\n",
      "Epoch [11/50], Train Loss: 0.1002, Train Accuracy: 0.9250, Validation Loss: 0.2055, Validation Accuracy: 0.9450\n",
      "Epoch [12/50], Train Loss: 0.0737, Train Accuracy: 0.9245, Validation Loss: 0.1879, Validation Accuracy: 0.9510\n",
      "Epoch [13/50], Train Loss: 0.1444, Train Accuracy: 0.9230, Validation Loss: 0.1796, Validation Accuracy: 0.9520\n",
      "Epoch [14/50], Train Loss: 0.1520, Train Accuracy: 0.9330, Validation Loss: 0.1630, Validation Accuracy: 0.9580\n",
      "Epoch [15/50], Train Loss: 0.0799, Train Accuracy: 0.9325, Validation Loss: 0.1756, Validation Accuracy: 0.9460\n",
      "Epoch [16/50], Train Loss: 0.1179, Train Accuracy: 0.9310, Validation Loss: 0.1534, Validation Accuracy: 0.9640\n",
      "Epoch [17/50], Train Loss: 0.3018, Train Accuracy: 0.9300, Validation Loss: 0.1745, Validation Accuracy: 0.9510\n",
      "Epoch [18/50], Train Loss: 0.0870, Train Accuracy: 0.9270, Validation Loss: 0.1457, Validation Accuracy: 0.9630\n",
      "Epoch [19/50], Train Loss: 0.0389, Train Accuracy: 0.9450, Validation Loss: 0.1590, Validation Accuracy: 0.9590\n",
      "Epoch [20/50], Train Loss: 0.1974, Train Accuracy: 0.9355, Validation Loss: 0.1295, Validation Accuracy: 0.9710\n",
      "Epoch [21/50], Train Loss: 0.1116, Train Accuracy: 0.9420, Validation Loss: 0.2022, Validation Accuracy: 0.9470\n",
      "Epoch [22/50], Train Loss: 0.1503, Train Accuracy: 0.9345, Validation Loss: 0.1792, Validation Accuracy: 0.9540\n",
      "Epoch [23/50], Train Loss: 0.1141, Train Accuracy: 0.9345, Validation Loss: 0.1489, Validation Accuracy: 0.9620\n",
      "Epoch [24/50], Train Loss: 0.0919, Train Accuracy: 0.9525, Validation Loss: 0.1729, Validation Accuracy: 0.9510\n",
      "Epoch [25/50], Train Loss: 0.0651, Train Accuracy: 0.9490, Validation Loss: 0.1862, Validation Accuracy: 0.9450\n",
      "Epoch [26/50], Train Loss: 0.4054, Train Accuracy: 0.9450, Validation Loss: 0.1665, Validation Accuracy: 0.9640\n",
      "Epoch [27/50], Train Loss: 0.2890, Train Accuracy: 0.9525, Validation Loss: 0.1329, Validation Accuracy: 0.9800\n",
      "Epoch [28/50], Train Loss: 0.1351, Train Accuracy: 0.9535, Validation Loss: 0.1805, Validation Accuracy: 0.9460\n",
      "Epoch [29/50], Train Loss: 0.0348, Train Accuracy: 0.9510, Validation Loss: 0.1200, Validation Accuracy: 0.9720\n",
      "Epoch [30/50], Train Loss: 0.0602, Train Accuracy: 0.9580, Validation Loss: 0.1107, Validation Accuracy: 0.9750\n",
      "Epoch [31/50], Train Loss: 0.0210, Train Accuracy: 0.9550, Validation Loss: 0.1230, Validation Accuracy: 0.9720\n",
      "Epoch [32/50], Train Loss: 0.0692, Train Accuracy: 0.9580, Validation Loss: 0.1452, Validation Accuracy: 0.9650\n",
      "Epoch [33/50], Train Loss: 0.0041, Train Accuracy: 0.9585, Validation Loss: 0.1227, Validation Accuracy: 0.9740\n",
      "Epoch [34/50], Train Loss: 0.3231, Train Accuracy: 0.9605, Validation Loss: 0.1340, Validation Accuracy: 0.9710\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Initialisation des paramètres d'entraînement\n",
    "steps_per_epoch = 100\n",
    "epochs = 50\n",
    "validation_steps = 50\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, steps_per_epoch, validation_steps):\n",
    "    for epoch in range(epochs):\n",
    "        accuracy = 0.0\n",
    "        running_loss = 0.0\n",
    "        # Entraîne le modèle sur les données d'entraînement\n",
    "        model.train()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Entraîne le modèle sur un lot de données\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.float().view(-1,1))\n",
    "            loss_values.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #running_loss += loss.item()\n",
    "            accuracy += (torch.round(outputs) == labels.float().view(-1,1)).sum().item() / len(labels)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            if (i+1) % steps_per_epoch == 0:\n",
    "                break\n",
    "        train_loss = running_loss / steps_per_epoch\n",
    "        train_acc = accuracy / steps_per_epoch \n",
    "                \n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            val_acc = 0\n",
    "            for i, (images, labels) in enumerate(val_loader):\n",
    "                outputs = model(images)\n",
    "                val_loss += criterion(outputs, labels.float().view(-1,1)).item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_acc += (torch.round(outputs) == labels.float().view(-1,1)).sum().item() / len(labels)\n",
    "                \n",
    "                if i >= validation_steps:\n",
    "                    break\n",
    "        \n",
    "        val_loss /= validation_steps\n",
    "        val_acc /= validation_steps \n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Lancer l'entraînement\n",
    "fit(model, train_loader, val_loader, epochs, steps_per_epoch, validation_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142affed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145b64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d670dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "step = np.linspace(0, 100, 10500)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(step, np.array(loss_values))\n",
    "plt.title(\"Step-wise Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac7b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdb3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(epoch_nums, training_acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epoch_nums, validation_acc, 'r', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epoch_nums, training_loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epoch_nums, validation_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed1907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42eb317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "\n",
    "layer_names = []\n",
    "for i in range(8):\n",
    "    layer_names.append(\"layer_{}\".format(i))\n",
    "images_per_row = 16\n",
    "\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    n_features = layer_activation.shape[1]\n",
    "    size = layer_activation.shape[2]\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = torch.zeros((n_cols * size, images_per_row * size))\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0, col * images_per_row + row, :, :]\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = channel_image.clamp(0, 255).to('uint8')\n",
    "            display_grid[col * size:(col + 1) * size, row * size:(row + 1) * size] = channel_image\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(vutils.make_grid(display_grid, normalize=False, scale_each=False), cmap='viridis')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
