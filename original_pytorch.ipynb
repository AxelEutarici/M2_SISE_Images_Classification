{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68f8027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import torchvision functions\n",
    "import torchvision\n",
    "from torchvision.models import vgg16\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "# import other functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, shutil\n",
    "from itertools import islice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "28d57285",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_dir = 'C:/Users/pauli/Documents/M2/ML et DL/projet/Git/DamageDetection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "907b809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_damage_dir = original_dataset_dir + '/train_another/damage'\n",
    "validation_damage_dir = original_dataset_dir + '/validation_another/damage'\n",
    "test_damage_dir = original_dataset_dir + '/test/damage'\n",
    "\n",
    "train_nodamage_dir = original_dataset_dir + '/train_another/no_damage'\n",
    "validation_nodamage_dir = original_dataset_dir + '/validation_another/no_damage'\n",
    "test_nodamage_dir = original_dataset_dir + '/test/no_damage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb3847ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training damage images:  5000\n",
      "total validation damage images:  1000\n",
      "total test damage images:  1000\n",
      "total training no damage images:  5000\n",
      "total validation no damage images:  1000\n",
      "total test no damage images:  1000\n"
     ]
    }
   ],
   "source": [
    "print('total training damage images: ',len(os.listdir(train_damage_dir)))\n",
    "print('total validation damage images: ',len(os.listdir(validation_damage_dir)))\n",
    "print('total test damage images: ',len(os.listdir(test_damage_dir)))\n",
    "\n",
    "print('total training no damage images: ',len(os.listdir(train_nodamage_dir)))\n",
    "print('total validation no damage images: ',len(os.listdir(validation_nodamage_dir)))\n",
    "print('total test no damage images: ',len(os.listdir(test_nodamage_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9fe3aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour entrainer les modeles\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, steps_per_epoch, validation_steps):\n",
    "    epoch_nums = []\n",
    "    training_acc = []\n",
    "    validation_acc = []\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        accuracy = 0.0\n",
    "        running_loss = 0.0\n",
    "        # Entraîne le modèle sur les données d'entraînement\n",
    "        model.train()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Entraîne le modèle sur un lot de données\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.float().view(-1,1))\n",
    "            loss_values.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #running_loss += loss.item()\n",
    "            accuracy += (torch.round(outputs) == labels.float().view(-1,1)).sum().item() / len(labels)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            if (i+1) % steps_per_epoch == 0:\n",
    "                break\n",
    "        train_loss = running_loss / steps_per_epoch\n",
    "        train_acc = accuracy / steps_per_epoch \n",
    "                \n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            val_acc = 0\n",
    "            for i, (images, labels) in enumerate(val_loader):\n",
    "                outputs = model(images)\n",
    "                val_loss += criterion(outputs, labels.float().view(-1,1)).item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_acc += (torch.round(outputs) == labels.float().view(-1,1)).sum().item() / len(labels)\n",
    "                \n",
    "                if i >= validation_steps:\n",
    "                    break\n",
    "        \n",
    "        val_loss /= validation_steps\n",
    "        val_acc /= validation_steps \n",
    "\n",
    "        epoch_nums.append(epoch+1)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(val_loss)\n",
    "        training_acc.append(train_acc)\n",
    "        validation_acc.append(val_acc)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "    return {\n",
    "        'training_acc': training_acc,\n",
    "        'validation_acc': validation_acc,\n",
    "        'training_loss': training_loss,\n",
    "        'validation_loss': validation_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1eced00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, (3,3))\n",
    "        self.pool1 = nn.MaxPool2d((2,2))\n",
    "        self.conv2 = nn.Conv2d(32, 64, (3,3))\n",
    "        self.pool2 = nn.MaxPool2d((2,2))\n",
    "        self.conv3 = nn.Conv2d(64, 128, (3,3))\n",
    "        self.pool3 = nn.MaxPool2d((2,2))\n",
    "        self.conv4 = nn.Conv2d(128, 128, (3,3))\n",
    "        self.pool4 = nn.MaxPool2d((2,2))\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 128 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ac90d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b637e282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 148, 148]             896\n",
      "         MaxPool2d-2           [-1, 32, 74, 74]               0\n",
      "            Conv2d-3           [-1, 64, 72, 72]          18,496\n",
      "         MaxPool2d-4           [-1, 64, 36, 36]               0\n",
      "            Conv2d-5          [-1, 128, 34, 34]          73,856\n",
      "         MaxPool2d-6          [-1, 128, 17, 17]               0\n",
      "            Conv2d-7          [-1, 128, 15, 15]         147,584\n",
      "         MaxPool2d-8            [-1, 128, 7, 7]               0\n",
      "            Linear-9                  [-1, 512]       3,211,776\n",
      "           Linear-10                    [-1, 1]             513\n",
      "================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.26\n",
      "Forward/backward pass size (MB): 11.53\n",
      "Params size (MB): 13.17\n",
      "Estimated Total Size (MB): 24.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "model = CNN()\n",
    "torchsummary.summary(model, (3,150,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "efe61c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a419d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    #transforms.Normalize((0,), (1/255,))\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder('C:/Users/pauli/Documents/M2/ML et DL/projet/Git/DamageDetection/train_another', transform=train_datagen)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "validation_datagen = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    #transforms.Normalize((0,), (1/255,))\n",
    "])\n",
    "\n",
    "val_dataset = ImageFolder('C:/Users/pauli/Documents/M2/ML et DL/projet/Git/DamageDetection/validation_another', transform=validation_datagen)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "09c09d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "print('Found {} images belonging to {} classes.'.format(len(train_dataset), len(train_dataset.classes)))\n",
    "print('Found {} images belonging to {} classes.'.format(len(val_dataset), len(val_dataset.classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "43792956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 0.4356, Train Accuracy: 0.6885, Validation Loss: 0.5844, Validation Accuracy: 0.6820\n",
      "Epoch [2/5], Train Loss: 0.1865, Train Accuracy: 0.8520, Validation Loss: 0.3699, Validation Accuracy: 0.8640\n",
      "Epoch [3/5], Train Loss: 0.5377, Train Accuracy: 0.8770, Validation Loss: 0.4801, Validation Accuracy: 0.7780\n",
      "Epoch [4/5], Train Loss: 0.4311, Train Accuracy: 0.8825, Validation Loss: 0.2284, Validation Accuracy: 0.9340\n",
      "Epoch [5/5], Train Loss: 0.3072, Train Accuracy: 0.8865, Validation Loss: 0.1973, Validation Accuracy: 0.9460\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialisation des paramètres \n",
    "steps_per_epoch = 100\n",
    "epochs = 5\n",
    "validation_steps = 50\n",
    "\n",
    "# Lancer l'entraînement\n",
    "history = fit(model, train_loader, val_loader, epochs, steps_per_epoch, validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dcdb3b7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5652\\4253523015.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'training_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvalidation_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'validation_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtraining_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'training_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'validation_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mepoch_nums\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "training_acc = history['training_acc']\n",
    "validation_acc = history['validation_acc']\n",
    "training_loss = history['training_loss']\n",
    "validation_loss = history['validation_loss']\n",
    "epoch_nums = range(1,len(training_acc)+1)\n",
    "\n",
    "plt.plot(epoch_nums, training_acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epoch_nums, validation_acc, 'r', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epoch_nums, training_loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epoch_nums, validation_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed1907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e12e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39082f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0d5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8394ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6b405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57939a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'C:/Users/pauli/Documents/M2/ML et DL/projet/Git/DamageDetection/damage/-96.960704_28.783292.jpeg'\n",
    "\n",
    "img = Image.open(img_path)\n",
    "img = img.resize((150,150))\n",
    "img_tensor = np.array(img)\n",
    "img_tensor = img_tensor.transpose((2, 0, 1))\n",
    "img_tensor = img_tensor[np.newaxis, :]\n",
    "img_tensor = img_tensor.astype('float32') / 255.\n",
    "\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e862640",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'C:/Users/pauli/Documents/M2/ML et DL/projet/Git/DamageDetection/damage/-96.960704_28.783292.jpeg'\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(150),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                 std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "img = Image.open(img_path)\n",
    "img_tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "# visualizing the tensor\n",
    "plt.imshow(img_tensor[0].permute(1, 2, 0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577660c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prendre un modele pre entrainé ou alors reprendre un model que j'avais deja aus dessus ... \n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "layer_outputs = [model.layer[i].output for i in range(8)]\n",
    "model_activations = torch.nn.Sequential(*layer_outputs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    activations = model_activations(img_tensor)\n",
    "\n",
    "first_layer_activation = activations[0]\n",
    "plt.matshow(first_layer_activation[0][0, :, :], cmap='viridis')\n",
    "\n",
    "second_layer_activation = activations[1]\n",
    "plt.matshow(second_layer_activation[0][0, :, :], cmap='viridis')\n",
    "\n",
    "third_layer_activation = activations[2]\n",
    "plt.matshow(third_layer_activation[0][0, :, :], cmap='viridis')\n",
    "\n",
    "layer_names = [str(i) for i in range(8)]\n",
    "images_per_row = 16\n",
    "\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    n_features = layer_activation.shape[1]\n",
    "    size = layer_activation.shape[2]\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = torch.zeros((size * n_cols, images_per_row * size))\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0, col * images_per_row + row, :, :]\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = channel_image.clamp(0, 255).type(torch.uint8)\n",
    "            display_grid[col * size:(col + 1) * size, row * size:(row + 1) * size] = channel_image\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid.permute(1, 0, 2), aspect='auto', cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626df44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6016b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
